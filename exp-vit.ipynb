{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":10046013,"sourceType":"datasetVersion","datasetId":6189052}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom tqdm import tqdm\nimport os\nimport math\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:00.838678Z","iopub.execute_input":"2024-12-02T00:20:00.839360Z","iopub.status.idle":"2024-12-02T00:20:07.272547Z","shell.execute_reply.started":"2024-12-02T00:20:00.839326Z","shell.execute_reply":"2024-12-02T00:20:07.271848Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# import torchvision\n# from torchvision import models, transforms\n# import timm\nfrom transformers import ViTModel, ViTFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:24:58.961683Z","iopub.execute_input":"2024-12-02T00:24:58.962086Z","iopub.status.idle":"2024-12-02T00:24:58.967190Z","shell.execute_reply.started":"2024-12-02T00:24:58.962050Z","shell.execute_reply":"2024-12-02T00:24:58.965945Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"lemmatizer\"])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:09.563695Z","iopub.execute_input":"2024-12-02T00:20:09.564086Z","iopub.status.idle":"2024-12-02T00:20:10.268917Z","shell.execute_reply.started":"2024-12-02T00:20:09.564044Z","shell.execute_reply":"2024-12-02T00:20:10.268007Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"ENVIRON = \"KAGGLE\"\nCONFIG = {\n    \"LOCAL\" : {\n        \"DF_PATH\": \"data/results.csv\",\n        \"IMAGES_DIR_ROOT\": \"data\",\n        \"FEATURE_MAPS_PATH\": \"feature_maps\",\n    },\n    \"KAGGLE\" : {\n        \"DF_PATH\": \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\",\n        \"IMAGES_DIR_ROOT\": \"/kaggle/input/flickr-image-dataset/flickr30k_images\",\n        \"FEATURE_MAPS_PATH\": \"/kaggle/input/feature-maps\"\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:10.269985Z","iopub.execute_input":"2024-12-02T00:20:10.270251Z","iopub.status.idle":"2024-12-02T00:20:10.274595Z","shell.execute_reply.started":"2024-12-02T00:20:10.270227Z","shell.execute_reply":"2024-12-02T00:20:10.273715Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Hyper Parameters","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-5\n\n# Model\nN_HEADS = 8\nN_LAYERS = 6\nEMBED_DIM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:10.276528Z","iopub.execute_input":"2024-12-02T00:20:10.276766Z","iopub.status.idle":"2024-12-02T00:20:10.287329Z","shell.execute_reply.started":"2024-12-02T00:20:10.276743Z","shell.execute_reply":"2024-12-02T00:20:10.286502Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"DF_PATH = CONFIG[ENVIRON][\"DF_PATH\"]\ndf = pd.read_csv(DF_PATH, delimiter=\"|\")\n\ndf.dropna(inplace=True)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:10.288377Z","iopub.execute_input":"2024-12-02T00:20:10.288918Z","iopub.status.idle":"2024-12-02T00:20:10.725344Z","shell.execute_reply.started":"2024-12-02T00:20:10.288882Z","shell.execute_reply":"2024-12-02T00:20:10.724488Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       image_name  comment_number  \\\n0  1000092795.jpg               0   \n1  1000092795.jpg               1   \n2  1000092795.jpg               2   \n3  1000092795.jpg               3   \n4  1000092795.jpg               4   \n\n                                             comment  \n0   Two young guys with shaggy hair look at their...  \n1   Two young , White males are outside near many...  \n2   Two men in green shirts are standing in a yard .  \n3       A man in a blue shirt standing in a garden .  \n4            Two friends enjoy time spent together .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>comment_number</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df[\"comment\"] = df[\" comment\"].apply(lambda x: str(x).strip()) \\\n                             .apply(lambda x: x.lower())\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:10.726693Z","iopub.execute_input":"2024-12-02T00:20:10.726976Z","iopub.status.idle":"2024-12-02T00:20:10.838791Z","shell.execute_reply.started":"2024-12-02T00:20:10.726928Z","shell.execute_reply":"2024-12-02T00:20:10.837826Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       image_name  comment_number  \\\n0  1000092795.jpg               0   \n1  1000092795.jpg               1   \n2  1000092795.jpg               2   \n3  1000092795.jpg               3   \n4  1000092795.jpg               4   \n\n                                             comment  \\\n0   Two young guys with shaggy hair look at their...   \n1   Two young , White males are outside near many...   \n2   Two men in green shirts are standing in a yard .   \n3       A man in a blue shirt standing in a garden .   \n4            Two friends enjoy time spent together .   \n\n                                             comment  \n0  two young guys with shaggy hair look at their ...  \n1  two young , white males are outside near many ...  \n2   two men in green shirts are standing in a yard .  \n3       a man in a blue shirt standing in a garden .  \n4            two friends enjoy time spent together .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>comment_number</th>\n      <th>comment</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their...</td>\n      <td>two young guys with shaggy hair look at their ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many...</td>\n      <td>two young , white males are outside near many ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n      <td>two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n      <td>a man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n      <td>two friends enjoy time spent together .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"captions_list = df[\"comment\"].to_list()\ntokens_list = []\ndocs = list(nlp.pipe(captions_list, n_process=-1))\n# for caption in tqdm(captions_list, desc=\"Vocab Building\"):\n#     tokens = nlp(caption)\n#     tokens = list(map(lambda x: x.text, tokens))\n#     vocab.update(tokens)\nfor doc in tqdm(docs, desc=\"Document Processing\"):\n    tokens = [token.text for token in doc]\n    tokens_list.append(tokens)\ndf[\"tokens\"] = tokens_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:20:10.840030Z","iopub.execute_input":"2024-12-02T00:20:10.840394Z","iopub.status.idle":"2024-12-02T00:23:50.955038Z","shell.execute_reply.started":"2024-12-02T00:20:10.840357Z","shell.execute_reply":"2024-12-02T00:23:50.954074Z"}},"outputs":[{"name":"stderr","text":"Document Processing: 100%|██████████| 158914/158914 [00:01<00:00, 149750.87it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"caption_length = df[\"tokens\"].apply(lambda x: len(x))\nmax_len = 15\nbool_map = (caption_length <= max_len)\nprint(\"No. of rows -\", df[bool_map].shape[0])\ndf = df[bool_map].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:50.956432Z","iopub.execute_input":"2024-12-02T00:23:50.957125Z","iopub.status.idle":"2024-12-02T00:23:51.091079Z","shell.execute_reply.started":"2024-12-02T00:23:50.957080Z","shell.execute_reply":"2024-12-02T00:23:51.090124Z"}},"outputs":[{"name":"stdout","text":"No. of rows - 115038\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Vocabulary","metadata":{}},{"cell_type":"code","source":"vocab = set()\ntokens_list = df[\"tokens\"].to_list()\nfor tokens in tqdm(tokens_list, desc=\"Vocab Building\"):\n    vocab.update(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.092134Z","iopub.execute_input":"2024-12-02T00:23:51.092397Z","iopub.status.idle":"2024-12-02T00:23:51.238191Z","shell.execute_reply.started":"2024-12-02T00:23:51.092371Z","shell.execute_reply":"2024-12-02T00:23:51.237360Z"}},"outputs":[{"name":"stderr","text":"Vocab Building: 100%|██████████| 115038/115038 [00:00<00:00, 999735.50it/s] \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"START_TOKEN = \"</start>\"\nEND_TOKEN = \"</end>\"\nPAD_TOKEN = \"</pad>\"\n\nvocab.add(START_TOKEN)\nvocab.add(END_TOKEN)\nvocab.add(PAD_TOKEN)\n\nvocab = sorted(list(vocab)) # Just incase\nvocab_size = len(vocab)\n\nprint(\"Vocab Size -\", vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.239438Z","iopub.execute_input":"2024-12-02T00:23:51.240089Z","iopub.status.idle":"2024-12-02T00:23:51.254198Z","shell.execute_reply.started":"2024-12-02T00:23:51.240049Z","shell.execute_reply":"2024-12-02T00:23:51.253409Z"}},"outputs":[{"name":"stdout","text":"Vocab Size - 14945\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"token_to_idx = {token: i for i, token in enumerate(vocab)}\nidx_to_token = {v: k for k, v in token_to_idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.255463Z","iopub.execute_input":"2024-12-02T00:23:51.256238Z","iopub.status.idle":"2024-12-02T00:23:51.266288Z","shell.execute_reply.started":"2024-12-02T00:23:51.256201Z","shell.execute_reply":"2024-12-02T00:23:51.265473Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# max_len += 2 # # +2 due to START and END tokens\nmax_len = 17 # +2 due to START and END tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.267128Z","iopub.execute_input":"2024-12-02T00:23:51.267359Z","iopub.status.idle":"2024-12-02T00:23:51.275490Z","shell.execute_reply.started":"2024-12-02T00:23:51.267336Z","shell.execute_reply":"2024-12-02T00:23:51.274766Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# df.drop(columns=[\"comment_number\", \"comment\"], inplace=True)\ndf[\"tokens\"] = df[\"tokens\"].apply(lambda x: [START_TOKEN, ] + x + [END_TOKEN, ])\ndf[\"tokens\"] = df[\"tokens\"].apply(lambda x: x + [PAD_TOKEN, ] * (max_len - len(x)))\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.279044Z","iopub.execute_input":"2024-12-02T00:23:51.279308Z","iopub.status.idle":"2024-12-02T00:23:51.975485Z","shell.execute_reply.started":"2024-12-02T00:23:51.279284Z","shell.execute_reply":"2024-12-02T00:23:51.974605Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"       image_name  comment_number  \\\n0  1000092795.jpg               1   \n1  1000092795.jpg               2   \n2  1000092795.jpg               3   \n3  1000092795.jpg               4   \n4    10002456.jpg               0   \n\n                                             comment  \\\n0   Two young , White males are outside near many...   \n1   Two men in green shirts are standing in a yard .   \n2       A man in a blue shirt standing in a garden .   \n3            Two friends enjoy time spent together .   \n4   Several men in hard hats are operating a gian...   \n\n                                             comment  \\\n0  two young , white males are outside near many ...   \n1   two men in green shirts are standing in a yard .   \n2       a man in a blue shirt standing in a garden .   \n3            two friends enjoy time spent together .   \n4  several men in hard hats are operating a giant...   \n\n                                              tokens  \n0  [</start>, two, young, ,, white, males, are, o...  \n1  [</start>, two, men, in, green, shirts, are, s...  \n2  [</start>, a, man, in, a, blue, shirt, standin...  \n3  [</start>, two, friends, enjoy, time, spent, t...  \n4  [</start>, several, men, in, hard, hats, are, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>comment_number</th>\n      <th>comment</th>\n      <th>comment</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many...</td>\n      <td>two young , white males are outside near many ...</td>\n      <td>[&lt;/start&gt;, two, young, ,, white, males, are, o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n      <td>two men in green shirts are standing in a yard .</td>\n      <td>[&lt;/start&gt;, two, men, in, green, shirts, are, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n      <td>a man in a blue shirt standing in a garden .</td>\n      <td>[&lt;/start&gt;, a, man, in, a, blue, shirt, standin...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n      <td>two friends enjoy time spent together .</td>\n      <td>[&lt;/start&gt;, two, friends, enjoy, time, spent, t...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10002456.jpg</td>\n      <td>0</td>\n      <td>Several men in hard hats are operating a gian...</td>\n      <td>several men in hard hats are operating a giant...</td>\n      <td>[&lt;/start&gt;, several, men, in, hard, hats, are, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"# df = df.sample(frac=.05).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.976584Z","iopub.execute_input":"2024-12-02T00:23:51.976890Z","iopub.status.idle":"2024-12-02T00:23:51.980568Z","shell.execute_reply.started":"2024-12-02T00:23:51.976862Z","shell.execute_reply":"2024-12-02T00:23:51.979737Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def get_train_test_split(df, test_size):\n    train_df, val_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    return train_df, val_df\ntrain_df, val_df = get_train_test_split(df, test_size=.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:51.981712Z","iopub.execute_input":"2024-12-02T00:23:51.982055Z","iopub.status.idle":"2024-12-02T00:23:52.058213Z","shell.execute_reply.started":"2024-12-02T00:23:51.982020Z","shell.execute_reply":"2024-12-02T00:23:52.057179Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"#### Old Implementation - Ignore (Don't Remove)","metadata":{}},{"cell_type":"code","source":"# class CaptionsDataset(Dataset):\n#     def __init__(self, df, token_to_idx, image_transforms):\n#         self.df = df\n#         self.token_to_idx = token_to_idx\n#         self.image_transforms = image_transforms\n#         resnet = models.resnet18(pretrained=True)\n#         self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n#     def __len__(self):\n#         return len(self.df)\n#     def _encode_tokens(self, tokens):\n#         return [self.token_to_idx[token] for token in tokens]\n#     def _get_image_features(self, x):\n#         with torch.no_grad():\n#             image_features = self.resnet(x.unsqueeze(0))\n#         return image_features.squeeze()\n#     def __getitem__(self, i):\n#         image_name, tokens = self.df.loc[i, \"image_name\"], self.df.loc[i, \"tokens\"]\n#         target_tokens = tokens[1:] + [PAD_TOKEN, ]\n#         image_path = os.path.join(\"data\", \"flickr30k_images\", image_name)\n#         image = Image.open(image_path)\n#         image = self.image_transforms(image)\n#         tokens = self._encode_tokens(tokens)\n#         target_tokens = self._encode_tokens(target_tokens)\n#         tokens = torch.tensor(tokens, dtype=torch.long)\n#         target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n#         image_features = self._get_image_features(image)\n#         return image_features, tokens, target_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.059533Z","iopub.execute_input":"2024-12-02T00:23:52.059803Z","iopub.status.idle":"2024-12-02T00:23:52.064011Z","shell.execute_reply.started":"2024-12-02T00:23:52.059768Z","shell.execute_reply":"2024-12-02T00:23:52.063159Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# train_dataset = CaptionsDataset(train_df, token_to_idx, transform)\n# val_dataset = CaptionsDataset(val_df, token_to_idx, transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n# a, b, c = next(iter(train_loader))\n# a.size(), b.size(), c.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.065160Z","iopub.execute_input":"2024-12-02T00:23:52.065454Z","iopub.status.idle":"2024-12-02T00:23:52.078479Z","shell.execute_reply.started":"2024-12-02T00:23:52.065424Z","shell.execute_reply":"2024-12-02T00:23:52.077700Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"#### New Implementation","metadata":{}},{"cell_type":"code","source":"# mean = [0.485, 0.456, 0.406]\n# std = [0.229, 0.224, 0.225]\n# transform = transforms.Compose([\n    \n#     transforms.Resize((224, 224)),\n#     transforms.ToTensor(),\n#     transforms.Normalize(mean=mean, std=std),\n# ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.079517Z","iopub.execute_input":"2024-12-02T00:23:52.080049Z","iopub.status.idle":"2024-12-02T00:23:52.089154Z","shell.execute_reply.started":"2024-12-02T00:23:52.080022Z","shell.execute_reply":"2024-12-02T00:23:52.088380Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# image_encoder_model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n# image_encoder_model = nn.Sequential(*list(image_encoder_model.children())[:-1]).to(device)\n\n# def get_feature_map(x):\n#     if x.ndimension() == 3:\n#         x = x.unsqueeze(0)\n#     with torch.no_grad():\n#         feature_map = image_encoder_model(x.to(device))\n#     if feature_map.ndimension() == 4:\n#         feature_map = feature_map.squeeze()\n#     # feature_map = feature_map.permute(1, 2, 0)\n#     # channels_size = feature_map.size()[-1]\n#     # feature_map = feature_map.view(1, -1, channels_size)\n#     return feature_map\n\n    \nvit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\nvit_feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n    \ndef get_feature_map(x):\n    if x.ndimension() == 3:\n        x = x.unsqueeze(0)\n    with torch.no_grad():\n        inputs = vit_feature_extractor(images=x, return_tensors=\"pt\")\n        outputs = vit_model(**inputs)\n    features = outputs.last_hidden_state\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:25:09.977192Z","iopub.execute_input":"2024-12-02T00:25:09.977519Z","iopub.status.idle":"2024-12-02T00:25:13.661835Z","shell.execute_reply.started":"2024-12-02T00:25:09.977490Z","shell.execute_reply":"2024-12-02T00:25:13.660824Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8433ece62e254bfd87af8381b2e4b977"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"# import pickle\n# image_names = set(df[\"image_name\"].to_list())\n# feature_maps = {}\n# for image_name in tqdm(image_names, desc=\"Swin Transformer Feature Maps\"):\n#     image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n#     image = Image.open(image_path)\n#     image = transform(image)\n#     feature_map = get_feature_map(image)\n#     feature_maps[image_name] = feature_map\n# feature_map_path = os.path.join(\"feature_maps\", \"resnet50\")\n# with open(feature_map_path, \"wb\") as f:\n#     pickle.dump(feature_maps, f)\n\nimport pickle\nimage_names = set(df[\"image_name\"].to_list())\nfeature_maps = {}\nfor image_name in tqdm(image_names, desc=\"ViT Feature Extractor\"):\n    image_path = os.path.join(\"data\", \"flickr30k_images\", image_name)\n    image = Image.open(image_path)\n    image = transform(image)\n    feature_map = get_feature_map(image)\n    feature_maps[image_name] = feature_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:25:16.213036Z","iopub.execute_input":"2024-12-02T00:25:16.213740Z"}},"outputs":[{"name":"stderr","text":"Swin Transformer Feature Maps:   1%|▏         | 469/31551 [00:17<18:19, 28.28it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"class CaptionsDataset(Dataset):\n    def __init__(self, df, token_to_idx, feature_maps=None):\n        \n        self.df = df\n        self.token_to_idx = token_to_idx\n        self.feature_maps = feature_maps\n        \n    def __len__(self):\n        return len(self.df)\n    def _encode_tokens(self, tokens):\n        return [self.token_to_idx[token] for token in tokens]\n    def _process_image(self, image_name):\n        image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n        image = Image.open(image_path)\n        image = transform(image)\n        image_features = get_feature_map(image)\n        return image_features\n    def __getitem__(self, i):\n        image_name, tokens = self.df.loc[i, \"image_name\"], self.df.loc[i, \"tokens\"]\n        target_tokens = tokens[1:] + [PAD_TOKEN, ]\n        tokens = self._encode_tokens(tokens)\n        target_tokens = self._encode_tokens(target_tokens)\n        tokens = torch.tensor(tokens, dtype=torch.long)\n        target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n        \n        if self.feature_maps is not None:\n            image_features = self.feature_maps[image_name]\n        else:\n            image_features = self._process_image(image_name)\n        image_features = image_features.squeeze()\n        \n        return image_features, tokens, target_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.366799Z","iopub.status.idle":"2024-12-02T00:23:52.367249Z","shell.execute_reply.started":"2024-12-02T00:23:52.367023Z","shell.execute_reply":"2024-12-02T00:23:52.367047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FEATURE_MAPS_FILENAME = os.path.join(CONFIG[ENVIRON][\"FEATURE_MAPS_PATH\"], \"vit\")\nfeature_maps = pd.read_pickle(FEATURE_MAPS_FILENAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.368803Z","iopub.status.idle":"2024-12-02T00:23:52.369261Z","shell.execute_reply.started":"2024-12-02T00:23:52.369039Z","shell.execute_reply":"2024-12-02T00:23:52.369062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_datasets(df, token_to_idx, feature_maps):\n    train_df, val_df = get_train_test_split(df, test_size=.2)\n    train_dataset = CaptionsDataset(train_df, token_to_idx, feature_maps)\n    val_dataset = CaptionsDataset(val_df, token_to_idx, feature_maps)\n    return train_dataset, val_dataset\n\ndef get_dataloaders(df, token_to_idx, feature_maps, batch_size):\n    train_dataset, val_dataset = get_datasets(df, token_to_idx, feature_maps)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.371101Z","iopub.status.idle":"2024-12-02T00:23:52.371530Z","shell.execute_reply.started":"2024-12-02T00:23:52.371308Z","shell.execute_reply":"2024-12-02T00:23:52.371331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, embed_dim, p=0.1, max_length=max_len):\n        super(PositionalEmbedding, self).__init__()\n        self.dropout_layer = nn.Dropout(p)\n        encoding = torch.zeros(max_length, embed_dim)\n        positions = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n        scale_factor = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        encoding[:, 0::2] = torch.sin(positions * scale_factor)\n        encoding[:, 1::2] = torch.cos(positions * scale_factor)\n        encoding = encoding.unsqueeze(0)\n        self.register_buffer('encoding', encoding)\n    def forward(self, x):\n        if self.encoding.size(0) < x.size(0):\n            self.encoding = self.encoding.repeat(x.size(0), 1, 1).to(device)\n\n        self.encoding = self.encoding[:x.size(0), :, :]\n\n        x = x + self.encoding\n\n        return self.dropout_layer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.372774Z","iopub.status.idle":"2024-12-02T00:23:52.373218Z","shell.execute_reply.started":"2024-12-02T00:23:52.372999Z","shell.execute_reply":"2024-12-02T00:23:52.373021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, n_heads, n_layers, vocab_size, embed_dim):\n        super(ImageCaptioningModel, self).__init__()\n\n        self.position_encoder = PositionalEmbedding(embed_dim, 0.1)\n\n        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=n_heads)\n        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer, num_layers=n_layers)\n        \n        self.embed_dim = embed_dim\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.output_layer = nn.Linear(embed_dim, vocab_size)\n\n        self.feature_map_reduce = nn.Linear(1024, EMBED_DIM)\n        \n        self._initialize_weights()\n\n    def _initialize_weights(self, param_range=0.1):\n        self.embedding.weight.data.uniform_(-param_range, param_range)\n        self.output_layer.bias.data.zero_()\n        self.output_layer.weight.data.uniform_(-param_range, param_range)\n\n    def _create_masks(self, size, decoder_input):\n        \n        causal_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        causal_mask = causal_mask.float().masked_fill(causal_mask == 0, float('-inf')).masked_fill(causal_mask == 1, float(0.0))\n\n        pad_mask = decoder_input.float().masked_fill(decoder_input == 0, float(0.0)).masked_fill(decoder_input > 0, float(1.0))\n        pad_mask_bool = decoder_input == 0\n\n        return causal_mask, pad_mask, pad_mask_bool\n\n    def forward(self, image_features, decoder_input):\n\n        image_features = image_features.permute(1, 0, 2) # (num_patches, batch_size, embed_dim) or (feature_map_size, batch_size, num_channels)\n        \n        decoder_input_embed = self.embedding(decoder_input) * math.sqrt(self.embed_dim)\n        decoder_input_embed = self.position_encoder(decoder_input_embed)\n        \n        decoder_input_embed = decoder_input_embed.permute(1, 0, 2)\n        causal_mask, pad_mask, pad_mask_bool = self._create_masks(decoder_input.size(1), decoder_input)\n\n        causal_mask = causal_mask.to(device)\n        pad_mask = pad_mask.to(device)\n        pad_mask_bool = pad_mask_bool.to(device)\n\n        decoder_output = self.decoder(tgt=decoder_input_embed, memory=image_features, tgt_mask=causal_mask, tgt_key_padding_mask=pad_mask_bool)\n        \n        output = self.output_layer(decoder_output)\n        return output, pad_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.374740Z","iopub.status.idle":"2024-12-02T00:23:52.375194Z","shell.execute_reply.started":"2024-12-02T00:23:52.374964Z","shell.execute_reply":"2024-12-02T00:23:52.374986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training & Validation Loop","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n\n    progress_bar = tqdm(train_loader, desc=\"Training\")\n    for image_features, tokens, target_tokens in progress_bar:\n\n        optimizer.zero_grad()\n        \n        image_features = image_features.to(device)\n        tokens = tokens.to(device)\n        target_tokens = target_tokens.to(device)\n        \n        logits, padding_mask = model(image_features, tokens)\n        logits = logits.permute(1, 2, 0)\n\n        loss = criterion(logits, target_tokens)\n        loss_masked = torch.mul(loss, padding_mask)\n\n        batch_loss = torch.sum(loss_masked) / torch.sum(padding_mask)\n        batch_loss.backward()\n        optimizer.step()\n\n        total_loss += batch_loss.item()\n\n        progress_bar.set_postfix(batch_loss=batch_loss.item(), refresh=True)\n\n    avg_loss = total_loss / len(train_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.377018Z","iopub.status.idle":"2024-12-02T00:23:52.377297Z","shell.execute_reply.started":"2024-12-02T00:23:52.377163Z","shell.execute_reply":"2024-12-02T00:23:52.377177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    \n    progress_bar = tqdm(val_loader, desc=\"Validation\")\n    with torch.inference_mode():\n        for image_features, tokens, target_tokens in progress_bar:\n            \n            image_features = image_features.to(device)\n            tokens = tokens.to(device)\n            target_tokens = target_tokens.to(device)\n\n            logits, padding_mask = model(image_features, tokens)\n            logits = logits.permute(1, 2, 0)\n\n            loss = criterion(logits, target_tokens)\n            loss_masked = torch.mul(loss, padding_mask)\n\n            batch_loss = torch.sum(loss_masked) / torch.sum(padding_mask)\n\n            total_loss += batch_loss.item()\n\n            progress_bar.set_postfix(batch_loss=batch_loss.item(), refresh=True)\n\n    avg_loss = total_loss / len(val_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.378307Z","iopub.status.idle":"2024-12-02T00:23:52.378606Z","shell.execute_reply.started":"2024-12-02T00:23:52.378460Z","shell.execute_reply":"2024-12-02T00:23:52.378475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n    \n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        print(f\"Epoch : [{epoch + 1}/{num_epochs}]\")\n\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss = val_epoch(model, val_loader, criterion, device)\n\n        print(f\"Training Loss: {train_loss:.4f}\")\n        print(f\"Validation Loss: {val_loss:.4f}\")\n\n        scheduler.step(val_loss)\n\n        for param_group in optimizer.param_groups:\n            print(f\"Current Learning Rate: {param_group['lr']}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model, \"best_model\")\n            print(\"Model saved!\")\n\n    print(\"Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.380137Z","iopub.status.idle":"2024-12-02T00:23:52.380556Z","shell.execute_reply.started":"2024-12-02T00:23:52.380341Z","shell.execute_reply":"2024-12-02T00:23:52.380363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\nmodel = ImageCaptioningModel(N_HEADS, N_LAYERS, vocab_size, EMBED_DIM).to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.3, patience=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.381590Z","iopub.status.idle":"2024-12-02T00:23:52.381856Z","shell.execute_reply.started":"2024-12-02T00:23:52.381723Z","shell.execute_reply":"2024-12-02T00:23:52.381737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader = get_dataloaders(df, token_to_idx, feature_maps, BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.383476Z","iopub.status.idle":"2024-12-02T00:23:52.383771Z","shell.execute_reply.started":"2024-12-02T00:23:52.383631Z","shell.execute_reply":"2024-12-02T00:23:52.383646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.384669Z","iopub.status.idle":"2024-12-02T00:23:52.384938Z","shell.execute_reply.started":"2024-12-02T00:23:52.384802Z","shell.execute_reply":"2024-12-02T00:23:52.384815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(K, image_name):\n    \n    image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n    image = Image.open(image_path).convert(\"RGB\")\n    plt.imshow(image)\n\n    model.eval()\n    feature_map = feature_maps[image_name].to(device)\n\n\n    input_tokens = [token_to_idx[PAD_TOKEN]] * max_len\n    input_tokens[0] = token_to_idx[START_TOKEN]\n\n    input_tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n    predicted_sentence = []\n    \n    with torch.no_grad():\n        for eval_iter in range(0, max_len-1):\n\n            logits, padding_mask = model.forward(feature_map, input_tokens)\n\n            logits = logits[eval_iter, 0, :]\n\n            values = torch.topk(logits, K).values.tolist()\n            indices = torch.topk(logits, K).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = idx_to_token[next_word_index]\n\n            input_tokens[:, eval_iter+1] = next_word_index\n\n\n            if next_word == '</end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    print(\"\\n\")\n    print(\"Predicted caption : \")\n    print(\" \".join(predicted_sentence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.386540Z","iopub.status.idle":"2024-12-02T00:23:52.386970Z","shell.execute_reply.started":"2024-12-02T00:23:52.386737Z","shell.execute_reply":"2024-12-02T00:23:52.386758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load(\"best_model\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.388369Z","iopub.status.idle":"2024-12-02T00:23:52.388789Z","shell.execute_reply.started":"2024-12-02T00:23:52.388570Z","shell.execute_reply":"2024-12-02T00:23:52.388594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.390018Z","iopub.status.idle":"2024-12-02T00:23:52.390436Z","shell.execute_reply.started":"2024-12-02T00:23:52.390222Z","shell.execute_reply":"2024-12-02T00:23:52.390243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.391828Z","iopub.status.idle":"2024-12-02T00:23:52.392274Z","shell.execute_reply.started":"2024-12-02T00:23:52.392044Z","shell.execute_reply":"2024-12-02T00:23:52.392066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.393309Z","iopub.status.idle":"2024-12-02T00:23:52.393808Z","shell.execute_reply.started":"2024-12-02T00:23:52.393554Z","shell.execute_reply":"2024-12-02T00:23:52.393581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.395697Z","iopub.status.idle":"2024-12-02T00:23:52.396005Z","shell.execute_reply.started":"2024-12-02T00:23:52.395837Z","shell.execute_reply":"2024-12-02T00:23:52.395852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.396971Z","iopub.status.idle":"2024-12-02T00:23:52.397276Z","shell.execute_reply.started":"2024-12-02T00:23:52.397137Z","shell.execute_reply":"2024-12-02T00:23:52.397152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.398208Z","iopub.status.idle":"2024-12-02T00:23:52.398498Z","shell.execute_reply.started":"2024-12-02T00:23:52.398358Z","shell.execute_reply":"2024-12-02T00:23:52.398373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.400099Z","iopub.status.idle":"2024-12-02T00:23:52.400394Z","shell.execute_reply.started":"2024-12-02T00:23:52.400254Z","shell.execute_reply":"2024-12-02T00:23:52.400268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-02T00:23:52.401395Z","iopub.status.idle":"2024-12-02T00:23:52.401657Z","shell.execute_reply.started":"2024-12-02T00:23:52.401528Z","shell.execute_reply":"2024-12-02T00:23:52.401541Z"}},"outputs":[],"execution_count":null}]}