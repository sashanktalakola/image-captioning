{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535e2669-85d5-48b5-b93d-dd489bafcfc2",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3489fe81-c874-416f-84a4-39f671a489ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from transformers import ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e0156b-7ae7-4c49-8d48-dc9a612491a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0130ee9a-f637-47b4-8d43-71e422cf4746",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49435bed-d05a-4382-9f57-c0caa3599f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name  comment_number  \\\n",
       "0  1000092795.jpg               0   \n",
       "1  1000092795.jpg               1   \n",
       "2  1000092795.jpg               2   \n",
       "3  1000092795.jpg               3   \n",
       "4  1000092795.jpg               4   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/results.csv\", delimiter=\"|\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e69284d-282e-49bf-97bd-85db5fdb4510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127132, 3), (31783, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=.2, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44faa5c4-e108-4226-a9e8-2e9bbf6ece7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file, transform=None, tokenizer=None, max_length=50):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.caption_file = caption_file\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.captions = self._load_captions()\n",
    "\n",
    "\n",
    "    def _load_captions(self):\n",
    "\n",
    "        if isinstance(self.caption_file, str):\n",
    "            df = pd.read_csv(self.caption_file, delimiter=\"|\")\n",
    "        else:\n",
    "            df = self.caption_file\n",
    "        return df\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        image_name = self.captions.loc[i, \"image_name\"]\n",
    "        caption = self.captions.loc[i, \" comment\"]\n",
    "\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            caption,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        caption_tokenized = encoding['input_ids'].squeeze(0)\n",
    "\n",
    "        return image, caption_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "854c6de7-5bf9-4d5e-aa50-55155376af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"data/flickr30k_images\"\n",
    "caption_file = \"data/results.csv\"\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "if not tokenizer.pad_token:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = Flickr30kDataset(image_dir, train_df, transform=transform, tokenizer=tokenizer, max_length=50)\n",
    "test_dataset = Flickr30kDataset(image_dir, test_df, transform=transform, tokenizer=tokenizer, max_length=50)\n",
    "\n",
    "train_laoder = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_laoder = DataLoader(train_dataset, batch_size=BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9354c0e8-ef70-4b9a-b001-0c6d33bdc4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32, 50]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = next(iter(train_laoder))\n",
    "\n",
    "a.size(), b.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e142db8-fbe9-4bad-856c-b94682f72728",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a329607e-1715-474c-af0b-cb90a8bc8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, use_all_tokens=True):\n",
    "        super(ViTFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.use_all_tokens = use_all_tokens\n",
    "        self.vit_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.vit_model(x).last_hidden_state\n",
    "\n",
    "        if self.use_all_tokens:\n",
    "            return features\n",
    "        else:\n",
    "            return features[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8954e4a3-9496-422f-acd5-3a8cab2f2e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 197, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor()\n",
    "\n",
    "feature_extractor(a).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
