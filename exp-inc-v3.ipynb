{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport spacy\nfrom tqdm import tqdm\nimport os\nimport math\nimport random\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:00.329173Z","iopub.execute_input":"2024-11-29T17:42:00.329884Z","iopub.status.idle":"2024-11-29T17:42:04.080039Z","shell.execute_reply.started":"2024-11-29T17:42:00.329837Z","shell.execute_reply":"2024-11-29T17:42:04.079323Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nfrom torchvision import models, transforms\nimport timm\n# from transformers import ViTModel, ViTFeatureExtractor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:04.081731Z","iopub.execute_input":"2024-11-29T17:42:04.082125Z","iopub.status.idle":"2024-11-29T17:42:05.182014Z","shell.execute_reply.started":"2024-11-29T17:42:04.082098Z","shell.execute_reply":"2024-11-29T17:42:05.181326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\", \"lemmatizer\"])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:05.182937Z","iopub.execute_input":"2024-11-29T17:42:05.183361Z","iopub.status.idle":"2024-11-29T17:42:05.882099Z","shell.execute_reply.started":"2024-11-29T17:42:05.183333Z","shell.execute_reply":"2024-11-29T17:42:05.881416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ENVIRON = \"KAGGLE\"\nCONFIG = {\n    \"LOCAL\" : {\n        \"DF_PATH\": \"data/results.csv\",\n        \"IMAGES_DIR_ROOT\": \"data\",\n        \"FEATURE_MAPS_PATH\": \"feature_maps\",\n    },\n    \"KAGGLE\" : {\n        \"DF_PATH\": \"/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv\",\n        \"IMAGES_DIR_ROOT\": \"/kaggle/input/flickr-image-dataset/flickr30k_images\",\n        \"FEATURE_MAPS_PATH\": \"/kaggle/input/feature-maps\"\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:05.883193Z","iopub.execute_input":"2024-11-29T17:42:05.883572Z","iopub.status.idle":"2024-11-29T17:42:05.88865Z","shell.execute_reply.started":"2024-11-29T17:42:05.883535Z","shell.execute_reply":"2024-11-29T17:42:05.887737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyper Parameters","metadata":{}},{"cell_type":"code","source":"EPOCHS = 10\nBATCH_SIZE = 32\nLEARNING_RATE = 1e-5\n\n# Model\nN_HEADS = 8\nN_LAYERS = 6\nEMBED_DIM = 128","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:49:01.304317Z","iopub.execute_input":"2024-11-29T17:49:01.305142Z","iopub.status.idle":"2024-11-29T17:49:01.309107Z","shell.execute_reply.started":"2024-11-29T17:49:01.305111Z","shell.execute_reply":"2024-11-29T17:49:01.308227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"DF_PATH = CONFIG[ENVIRON][\"DF_PATH\"]\ndf = pd.read_csv(DF_PATH, delimiter=\"|\")\n\ndf.dropna(inplace=True)\n\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:05.901455Z","iopub.execute_input":"2024-11-29T17:42:05.901676Z","iopub.status.idle":"2024-11-29T17:42:06.264136Z","shell.execute_reply.started":"2024-11-29T17:42:05.901653Z","shell.execute_reply":"2024-11-29T17:42:06.263284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"comment\"] = df[\" comment\"].apply(lambda x: str(x).strip()) \\\n                             .apply(lambda x: x.lower())\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:06.265464Z","iopub.execute_input":"2024-11-29T17:42:06.265813Z","iopub.status.idle":"2024-11-29T17:42:06.379387Z","shell.execute_reply.started":"2024-11-29T17:42:06.265776Z","shell.execute_reply":"2024-11-29T17:42:06.378746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"captions_list = df[\"comment\"].to_list()\ntokens_list = []\ndocs = list(nlp.pipe(captions_list, n_process=-1))\n# for caption in tqdm(captions_list, desc=\"Vocab Building\"):\n#     tokens = nlp(caption)\n#     tokens = list(map(lambda x: x.text, tokens))\n#     vocab.update(tokens)\nfor doc in tqdm(docs, desc=\"Document Processing\"):\n    tokens = [token.text for token in doc]\n    tokens_list.append(tokens)\ndf[\"tokens\"] = tokens_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:42:06.38069Z","iopub.execute_input":"2024-11-29T17:42:06.380951Z","iopub.status.idle":"2024-11-29T17:45:47.685765Z","shell.execute_reply.started":"2024-11-29T17:42:06.380926Z","shell.execute_reply":"2024-11-29T17:45:47.684807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"caption_length = df[\"tokens\"].apply(lambda x: len(x))\nmax_len = 15\nbool_map = (caption_length <= max_len)\nprint(\"No. of rows -\", df[bool_map].shape[0])\ndf = df[bool_map].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:47.687179Z","iopub.execute_input":"2024-11-29T17:45:47.687602Z","iopub.status.idle":"2024-11-29T17:45:47.827872Z","shell.execute_reply.started":"2024-11-29T17:45:47.687559Z","shell.execute_reply":"2024-11-29T17:45:47.826935Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Vocabulary","metadata":{}},{"cell_type":"code","source":"vocab = set()\ntokens_list = df[\"tokens\"].to_list()\nfor tokens in tqdm(tokens_list, desc=\"Vocab Building\"):\n    vocab.update(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:47.828925Z","iopub.execute_input":"2024-11-29T17:45:47.829191Z","iopub.status.idle":"2024-11-29T17:45:47.972271Z","shell.execute_reply.started":"2024-11-29T17:45:47.829165Z","shell.execute_reply":"2024-11-29T17:45:47.97146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"START_TOKEN = \"</start>\"\nEND_TOKEN = \"</end>\"\nPAD_TOKEN = \"</pad>\"\n\nvocab.add(START_TOKEN)\nvocab.add(END_TOKEN)\nvocab.add(PAD_TOKEN)\n\nvocab = sorted(list(vocab)) # Just incase\nvocab_size = len(vocab)\n\nprint(\"Vocab Size -\", vocab_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:47.973249Z","iopub.execute_input":"2024-11-29T17:45:47.973524Z","iopub.status.idle":"2024-11-29T17:45:47.987597Z","shell.execute_reply.started":"2024-11-29T17:45:47.973498Z","shell.execute_reply":"2024-11-29T17:45:47.986706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"token_to_idx = {token: i for i, token in enumerate(vocab)}\nidx_to_token = {v: k for k, v in token_to_idx.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:47.988644Z","iopub.execute_input":"2024-11-29T17:45:47.98895Z","iopub.status.idle":"2024-11-29T17:45:48.003646Z","shell.execute_reply.started":"2024-11-29T17:45:47.988916Z","shell.execute_reply":"2024-11-29T17:45:48.002831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# max_len += 2 # # +2 due to START and END tokens\nmax_len = 17 # +2 due to START and END tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.004555Z","iopub.execute_input":"2024-11-29T17:45:48.004768Z","iopub.status.idle":"2024-11-29T17:45:48.012923Z","shell.execute_reply.started":"2024-11-29T17:45:48.004747Z","shell.execute_reply":"2024-11-29T17:45:48.012174Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.drop(columns=[\"comment_number\", \"comment\"], inplace=True)\ndf[\"tokens\"] = df[\"tokens\"].apply(lambda x: [START_TOKEN, ] + x + [END_TOKEN, ])\ndf[\"tokens\"] = df[\"tokens\"].apply(lambda x: x + [PAD_TOKEN, ] * (max_len - len(x)))\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.017224Z","iopub.execute_input":"2024-11-29T17:45:48.017565Z","iopub.status.idle":"2024-11-29T17:45:48.734677Z","shell.execute_reply.started":"2024-11-29T17:45:48.017527Z","shell.execute_reply":"2024-11-29T17:45:48.733729Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"# df = df.sample(frac=.05).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.736063Z","iopub.execute_input":"2024-11-29T17:45:48.736518Z","iopub.status.idle":"2024-11-29T17:45:48.74162Z","shell.execute_reply.started":"2024-11-29T17:45:48.73647Z","shell.execute_reply":"2024-11-29T17:45:48.740488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_train_test_split(df, test_size):\n    train_df, val_df = train_test_split(df, test_size=test_size, random_state=42)\n    \n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n    return train_df, val_df\ntrain_df, val_df = get_train_test_split(df, test_size=.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.743075Z","iopub.execute_input":"2024-11-29T17:45:48.743737Z","iopub.status.idle":"2024-11-29T17:45:48.823082Z","shell.execute_reply.started":"2024-11-29T17:45:48.743706Z","shell.execute_reply":"2024-11-29T17:45:48.822344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Old Implementation - Ignore (Don't Remove)","metadata":{}},{"cell_type":"code","source":"# class CaptionsDataset(Dataset):\n#     def __init__(self, df, token_to_idx, image_transforms):\n#         self.df = df\n#         self.token_to_idx = token_to_idx\n#         self.image_transforms = image_transforms\n#         resnet = models.resnet18(pretrained=True)\n#         self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n#     def __len__(self):\n#         return len(self.df)\n#     def _encode_tokens(self, tokens):\n#         return [self.token_to_idx[token] for token in tokens]\n#     def _get_image_features(self, x):\n#         with torch.no_grad():\n#             image_features = self.resnet(x.unsqueeze(0))\n#         return image_features.squeeze()\n#     def __getitem__(self, i):\n#         image_name, tokens = self.df.loc[i, \"image_name\"], self.df.loc[i, \"tokens\"]\n#         target_tokens = tokens[1:] + [PAD_TOKEN, ]\n#         image_path = os.path.join(\"data\", \"flickr30k_images\", image_name)\n#         image = Image.open(image_path)\n#         image = self.image_transforms(image)\n#         tokens = self._encode_tokens(tokens)\n#         target_tokens = self._encode_tokens(target_tokens)\n#         tokens = torch.tensor(tokens, dtype=torch.long)\n#         target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n#         image_features = self._get_image_features(image)\n#         return image_features, tokens, target_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.824092Z","iopub.execute_input":"2024-11-29T17:45:48.824333Z","iopub.status.idle":"2024-11-29T17:45:48.829048Z","shell.execute_reply.started":"2024-11-29T17:45:48.824308Z","shell.execute_reply":"2024-11-29T17:45:48.828195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# train_dataset = CaptionsDataset(train_df, token_to_idx, transform)\n# val_dataset = CaptionsDataset(val_df, token_to_idx, transform)\n\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n# a, b, c = next(iter(train_loader))\n# a.size(), b.size(), c.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.830176Z","iopub.execute_input":"2024-11-29T17:45:48.830472Z","iopub.status.idle":"2024-11-29T17:45:48.841585Z","shell.execute_reply.started":"2024-11-29T17:45:48.830432Z","shell.execute_reply":"2024-11-29T17:45:48.840913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### New Implementation","metadata":{}},{"cell_type":"code","source":"mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n    \n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.842678Z","iopub.execute_input":"2024-11-29T17:45:48.84296Z","iopub.status.idle":"2024-11-29T17:45:48.851123Z","shell.execute_reply.started":"2024-11-29T17:45:48.842935Z","shell.execute_reply":"2024-11-29T17:45:48.850428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_encoder_model = timm.create_model('inception_v3', pretrained=True)\nimage_encoder_model = nn.Sequential(*list(image_encoder_model.children())[:-5]).to(device)\n\ndef get_feature_map(x):\n    if x.ndimension() == 3:\n        x = x.unsqueeze(0)\n    with torch.no_grad():\n        feature_map = image_encoder_model(x.to(device))\n    if feature_map.ndimension() == 4:\n        feature_map = feature_map.squeeze()\n    feature_map = feature_map.permute(1, 2, 0)\n    # channels_size = feature_map.size()[-1]\n    # feature_map = feature_map.view(1, -1, channels_size)\n    return feature_map\n\n    \n# vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n# vit_feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n    \n# def get_feature_map(x):\n#     if x.ndimension() == 3:\n#         x = x.unsqueeze(0)\n#     with torch.no_grad():\n#         inputs = vit_feature_extractor(images=x, return_tensors=\"pt\")\n#         outputs = vit_model(**inputs)\n#     features = outputs.last_hidden_state\n#     return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.852073Z","iopub.execute_input":"2024-11-29T17:45:48.852314Z","iopub.status.idle":"2024-11-29T17:45:48.860697Z","shell.execute_reply.started":"2024-11-29T17:45:48.852289Z","shell.execute_reply":"2024-11-29T17:45:48.859984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pickle\nimage_names = set(df[\"image_name\"].to_list())\nfeature_maps = {}\nfor image_name in tqdm(image_names, desc=\"Inception-v3 Feature Maps\"):\n    image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n    image = Image.open(image_path)\n    image = transform(image)\n    feature_map = get_feature_map(image)\n    feature_maps[image_name] = feature_map\n# feature_map_path = os.path.join(\"feature_maps\", \"resnet50\")\n# with open(feature_map_path, \"wb\") as f:\n#     pickle.dump(feature_maps, f)\n\n# import pickle\n# image_names = set(df[\"image_name\"].to_list())\n# feature_maps = {}\n# for image_name in tqdm(image_names, desc=\"ViT Feature Extractor\"):\n#     image_path = os.path.join(\"data\", \"flickr30k_images\", image_name)\n#     image = Image.open(image_path)\n#     image = transform(image)\n#     feature_map = get_feature_map(image)\n#     feature_maps[image_name] = feature_map","metadata":{"execution":{"iopub.status.busy":"2024-11-29T17:45:48.861707Z","iopub.execute_input":"2024-11-29T17:45:48.861942Z","iopub.status.idle":"2024-11-29T17:45:48.873828Z","shell.execute_reply.started":"2024-11-29T17:45:48.861919Z","shell.execute_reply":"2024-11-29T17:45:48.873175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CaptionsDataset(Dataset):\n    def __init__(self, df, token_to_idx, feature_maps=None):\n        \n        self.df = df\n        self.token_to_idx = token_to_idx\n        self.feature_maps = feature_maps\n        \n    def __len__(self):\n        return len(self.df)\n    def _encode_tokens(self, tokens):\n        return [self.token_to_idx[token] for token in tokens]\n    def _process_image(self, image_name):\n        image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n        image = Image.open(image_path)\n        image = transform(image)\n        image_features = get_feature_map(image)\n        return image_features\n    def __getitem__(self, i):\n        image_name, tokens = self.df.loc[i, \"image_name\"], self.df.loc[i, \"tokens\"]\n        target_tokens = tokens[1:] + [PAD_TOKEN, ]\n        tokens = self._encode_tokens(tokens)\n        target_tokens = self._encode_tokens(target_tokens)\n        tokens = torch.tensor(tokens, dtype=torch.long)\n        target_tokens = torch.tensor(target_tokens, dtype=torch.long)\n        \n        if self.feature_maps is not None:\n            image_features = self.feature_maps[image_name]\n        else:\n            image_features = self._process_image(image_name)\n        image_features = image_features.squeeze()\n        \n        return image_features, tokens, target_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:48:34.821461Z","iopub.execute_input":"2024-11-29T17:48:34.821805Z","iopub.status.idle":"2024-11-29T17:48:34.829522Z","shell.execute_reply.started":"2024-11-29T17:48:34.821775Z","shell.execute_reply":"2024-11-29T17:48:34.828683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FEATURE_MAPS_FILENAME = os.path.join(CONFIG[ENVIRON][\"FEATURE_MAPS_PATH\"], \"resnet18\")\n# feature_maps = pd.read_pickle(FEATURE_MAPS_FILENAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:45:48.886113Z","iopub.execute_input":"2024-11-29T17:45:48.886455Z","iopub.status.idle":"2024-11-29T17:46:14.721266Z","shell.execute_reply.started":"2024-11-29T17:45:48.886417Z","shell.execute_reply":"2024-11-29T17:46:14.720544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_datasets(df, token_to_idx, feature_maps):\n    train_df, val_df = get_train_test_split(df, test_size=.2)\n    train_dataset = CaptionsDataset(train_df, token_to_idx, feature_maps)\n    val_dataset = CaptionsDataset(val_df, token_to_idx, feature_maps)\n    return train_dataset, val_dataset\n\ndef get_dataloaders(df, token_to_idx, feature_maps, batch_size):\n    train_dataset, val_dataset = get_datasets(df, token_to_idx, feature_maps)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.726772Z","iopub.execute_input":"2024-11-29T17:46:14.727007Z","iopub.status.idle":"2024-11-29T17:46:14.732306Z","shell.execute_reply.started":"2024-11-29T17:46:14.726983Z","shell.execute_reply":"2024-11-29T17:46:14.731351Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(nn.Module):\n    def __init__(self, embed_dim, p=0.1, max_length=max_len):\n        super(PositionalEmbedding, self).__init__()\n        self.dropout_layer = nn.Dropout(p)\n        encoding = torch.zeros(max_length, embed_dim)\n        positions = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n        scale_factor = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        encoding[:, 0::2] = torch.sin(positions * scale_factor)\n        encoding[:, 1::2] = torch.cos(positions * scale_factor)\n        encoding = encoding.unsqueeze(0)\n        self.register_buffer('encoding', encoding)\n    def forward(self, x):\n        if self.encoding.size(0) < x.size(0):\n            self.encoding = self.encoding.repeat(x.size(0), 1, 1).to(device)\n\n        self.encoding = self.encoding[:x.size(0), :, :]\n\n        x = x + self.encoding\n\n        return self.dropout_layer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.733463Z","iopub.execute_input":"2024-11-29T17:46:14.733712Z","iopub.status.idle":"2024-11-29T17:46:14.747498Z","shell.execute_reply.started":"2024-11-29T17:46:14.733687Z","shell.execute_reply":"2024-11-29T17:46:14.746635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, n_heads, n_layers, vocab_size, embed_dim):\n        super(ImageCaptioningModel, self).__init__()\n\n        self.position_encoder = PositionalEmbedding(embed_dim, 0.1)\n\n        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=n_heads)\n        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer, num_layers=n_layers)\n        \n        self.embed_dim = embed_dim\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.output_layer = nn.Linear(embed_dim, vocab_size)\n\n        self.feature_map_reduce = nn.Linear(1280, EMBED_DIM)\n        \n        self._initialize_weights()\n\n    def _initialize_weights(self, param_range=0.1):\n        self.embedding.weight.data.uniform_(-param_range, param_range)\n        self.output_layer.bias.data.zero_()\n        self.output_layer.weight.data.uniform_(-param_range, param_range)\n\n    def _create_masks(self, size, decoder_input):\n        \n        causal_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        causal_mask = causal_mask.float().masked_fill(causal_mask == 0, float('-inf')).masked_fill(causal_mask == 1, float(0.0))\n\n        pad_mask = decoder_input.float().masked_fill(decoder_input == 0, float(0.0)).masked_fill(decoder_input > 0, float(1.0))\n        pad_mask_bool = decoder_input == 0\n\n        return causal_mask, pad_mask, pad_mask_bool\n\n    def forward(self, image_features, decoder_input):\n\n        batch_size, *_, channels_size = image_features.size()\n        image_features = image_features.view(batch_size, 1, -1, channels_size)\n        image_features = self.feature_map_reduce(image_features)\n        image_features = image_features.permute(1, 0, 2) # (num_patches, batch_size, embed_dim) or (feature_map_size, batch_size, num_channels)\n        \n        decoder_input_embed = self.embedding(decoder_input) * math.sqrt(self.embed_dim)\n        decoder_input_embed = self.position_encoder(decoder_input_embed)\n        \n        decoder_input_embed = decoder_input_embed.permute(1, 0, 2)\n        causal_mask, pad_mask, pad_mask_bool = self._create_masks(decoder_input.size(1), decoder_input)\n\n        causal_mask = causal_mask.to(device)\n        pad_mask = pad_mask.to(device)\n        pad_mask_bool = pad_mask_bool.to(device)\n\n        decoder_output = self.decoder(tgt=decoder_input_embed, memory=image_features, tgt_mask=causal_mask, tgt_key_padding_mask=pad_mask_bool)\n        \n        output = self.output_layer(decoder_output)\n        return output, pad_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.748534Z","iopub.execute_input":"2024-11-29T17:46:14.748767Z","iopub.status.idle":"2024-11-29T17:46:14.759248Z","shell.execute_reply.started":"2024-11-29T17:46:14.748743Z","shell.execute_reply":"2024-11-29T17:46:14.758355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training & Validation Loop","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0.0\n\n    progress_bar = tqdm(train_loader, desc=\"Training\")\n    for image_features, tokens, target_tokens in progress_bar:\n\n        optimizer.zero_grad()\n        \n        image_features = image_features.to(device)\n        tokens = tokens.to(device)\n        target_tokens = target_tokens.to(device)\n        \n        logits, padding_mask = model(image_features, tokens)\n        logits = logits.permute(1, 2, 0)\n\n        loss = criterion(logits, target_tokens)\n        loss_masked = torch.mul(loss, padding_mask)\n\n        batch_loss = torch.sum(loss_masked) / torch.sum(padding_mask)\n        batch_loss.backward()\n        optimizer.step()\n\n        total_loss += batch_loss.item()\n\n        progress_bar.set_postfix(batch_loss=batch_loss.item(), refresh=True)\n\n    avg_loss = total_loss / len(train_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.760305Z","iopub.execute_input":"2024-11-29T17:46:14.760555Z","iopub.status.idle":"2024-11-29T17:46:14.773681Z","shell.execute_reply.started":"2024-11-29T17:46:14.760532Z","shell.execute_reply":"2024-11-29T17:46:14.772898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val_epoch(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    \n    progress_bar = tqdm(val_loader, desc=\"Validation\")\n    with torch.inference_mode():\n        for image_features, tokens, target_tokens in progress_bar:\n            \n            image_features = image_features.to(device)\n            tokens = tokens.to(device)\n            target_tokens = target_tokens.to(device)\n\n            logits, padding_mask = model(image_features, tokens)\n            logits = logits.permute(1, 2, 0)\n\n            loss = criterion(logits, target_tokens)\n            loss_masked = torch.mul(loss, padding_mask)\n\n            batch_loss = torch.sum(loss_masked) / torch.sum(padding_mask)\n\n            total_loss += batch_loss.item()\n\n            progress_bar.set_postfix(batch_loss=batch_loss.item(), refresh=True)\n\n    avg_loss = total_loss / len(val_loader)\n    return avg_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.774716Z","iopub.execute_input":"2024-11-29T17:46:14.774962Z","iopub.status.idle":"2024-11-29T17:46:14.788882Z","shell.execute_reply.started":"2024-11-29T17:46:14.774938Z","shell.execute_reply":"2024-11-29T17:46:14.788179Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n    \n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        print(f\"Epoch : [{epoch + 1}/{num_epochs}]\")\n\n        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n        val_loss = val_epoch(model, val_loader, criterion, device)\n\n        print(f\"Training Loss: {train_loss:.4f}\")\n        print(f\"Validation Loss: {val_loss:.4f}\")\n\n        scheduler.step(val_loss)\n\n        for param_group in optimizer.param_groups:\n            print(f\"Current Learning Rate: {param_group['lr']}\")\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model, \"best_model\")\n            print(\"Model saved!\")\n\n    print(\"Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:14.789718Z","iopub.execute_input":"2024-11-29T17:46:14.789942Z","iopub.status.idle":"2024-11-29T17:46:14.797928Z","shell.execute_reply.started":"2024-11-29T17:46:14.789919Z","shell.execute_reply":"2024-11-29T17:46:14.797154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\nmodel = ImageCaptioningModel(N_HEADS, N_LAYERS, vocab_size, EMBED_DIM).to(device)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=.3, patience=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:55:07.880087Z","iopub.execute_input":"2024-11-29T17:55:07.88088Z","iopub.status.idle":"2024-11-29T17:55:08.26714Z","shell.execute_reply.started":"2024-11-29T17:55:07.880844Z","shell.execute_reply":"2024-11-29T17:55:08.266459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader, val_loader = get_dataloaders(df, token_to_idx, feature_maps, BATCH_SIZE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:54:51.413211Z","iopub.execute_input":"2024-11-29T17:54:51.414028Z","iopub.status.idle":"2024-11-29T17:54:51.500956Z","shell.execute_reply.started":"2024-11-29T17:54:51.413994Z","shell.execute_reply":"2024-11-29T17:54:51.499951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, EPOCHS, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:55:10.194858Z","iopub.execute_input":"2024-11-29T17:55:10.195448Z","iopub.status.idle":"2024-11-29T17:55:16.405035Z","shell.execute_reply.started":"2024-11-29T17:55:10.195378Z","shell.execute_reply":"2024-11-29T17:55:16.403848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_caption(K, image_name):\n    \n    image_path = os.path.join(CONFIG[ENVIRON][\"IMAGES_DIR_ROOT\"], \"flickr30k_images\", image_name)\n    image = Image.open(image_path).convert(\"RGB\")\n    plt.imshow(image)\n\n    model.eval()\n    feature_map = feature_maps[image_name].to(device)\n\n\n    input_tokens = [token_to_idx[PAD_TOKEN]] * max_len\n    input_tokens[0] = token_to_idx[START_TOKEN]\n\n    input_tokens = torch.tensor(input_tokens).unsqueeze(0).to(device)\n    predicted_sentence = []\n    \n    with torch.no_grad():\n        for eval_iter in range(0, max_len-1):\n\n            logits, padding_mask = model.forward(feature_map, input_tokens)\n\n            logits = logits[eval_iter, 0, :]\n\n            values = torch.topk(logits, K).values.tolist()\n            indices = torch.topk(logits, K).indices.tolist()\n\n            next_word_index = random.choices(indices, values, k = 1)[0]\n\n            next_word = idx_to_token[next_word_index]\n\n            input_tokens[:, eval_iter+1] = next_word_index\n\n\n            if next_word == '</end>' :\n                break\n\n            predicted_sentence.append(next_word)\n    print(\"\\n\")\n    print(\"Predicted caption : \")\n    print(\" \".join(predicted_sentence))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.796898Z","iopub.status.idle":"2024-11-29T17:46:17.797219Z","shell.execute_reply.started":"2024-11-29T17:46:17.797061Z","shell.execute_reply":"2024-11-29T17:46:17.797077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = torch.load(\"best_model\").to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.798675Z","iopub.status.idle":"2024-11-29T17:46:17.798939Z","shell.execute_reply.started":"2024-11-29T17:46:17.798809Z","shell.execute_reply":"2024-11-29T17:46:17.798822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.79983Z","iopub.status.idle":"2024-11-29T17:46:17.800114Z","shell.execute_reply.started":"2024-11-29T17:46:17.799977Z","shell.execute_reply":"2024-11-29T17:46:17.799992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.800948Z","iopub.status.idle":"2024-11-29T17:46:17.801214Z","shell.execute_reply.started":"2024-11-29T17:46:17.801084Z","shell.execute_reply":"2024-11-29T17:46:17.801097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.802224Z","iopub.status.idle":"2024-11-29T17:46:17.802554Z","shell.execute_reply.started":"2024-11-29T17:46:17.802372Z","shell.execute_reply":"2024-11-29T17:46:17.802387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.803962Z","iopub.status.idle":"2024-11-29T17:46:17.804263Z","shell.execute_reply.started":"2024-11-29T17:46:17.804111Z","shell.execute_reply":"2024-11-29T17:46:17.804126Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.805143Z","iopub.status.idle":"2024-11-29T17:46:17.805463Z","shell.execute_reply.started":"2024-11-29T17:46:17.805295Z","shell.execute_reply":"2024-11-29T17:46:17.80531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.807253Z","iopub.status.idle":"2024-11-29T17:46:17.807568Z","shell.execute_reply.started":"2024-11-29T17:46:17.807387Z","shell.execute_reply":"2024-11-29T17:46:17.807431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.808968Z","iopub.status.idle":"2024-11-29T17:46:17.809233Z","shell.execute_reply.started":"2024-11-29T17:46:17.809103Z","shell.execute_reply":"2024-11-29T17:46:17.809116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i += 1\ngenerate_caption(K=1, image_name=val_df.loc[i, \"image_name\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:46:17.810672Z","iopub.status.idle":"2024-11-29T17:46:17.810964Z","shell.execute_reply.started":"2024-11-29T17:46:17.810823Z","shell.execute_reply":"2024-11-29T17:46:17.810838Z"}},"outputs":[],"execution_count":null}]}